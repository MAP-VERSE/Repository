<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cartography on MAP-VERSE</title><link>https://MAP-VERSE.github.io/Repository/tags/cartography/</link><description>Recent content in Cartography on MAP-VERSE</description><generator>Hugo</generator><language>en</language><atom:link href="https://MAP-VERSE.github.io/Repository/tags/cartography/index.xml" rel="self" type="application/rss+xml"/><item><title>EyeCatchingMaps</title><link>https://MAP-VERSE.github.io/Repository/post/dataset05/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://MAP-VERSE.github.io/Repository/post/dataset05/</guid><description>&lt;h4 id="abstract">Abstract&lt;/h4>
&lt;p>&lt;a href="https://agile-giss.copernicus.org/articles/5/51/2024/">Saliency models try to predict the gaze behaviour of people in the first seconds of their observation of an image. To assess how much these models can perform to predict saliency in maps, we lack a ground truth to compare to. This paper proposes EyeCatchingMaps, an open dataset that can be used to benchmark saliency models for maps. The dataset has been obtained by recording the gaze of participants looking at different maps for 3 seconds with an eye-tracker. The use of EyeCatchingMaps is demonstrated by comparing two different saliency models from the literature to the real saliency maps derived from people’s gaze.&lt;/a>

&lt;link rel="stylesheet" href="https://MAP-VERSE.github.io/Repository/css/hugo-easy-gallery.css" />

&lt;div class="gallery caption-position-bottom caption-effect-fade hover-effect-zoom hover-transition" itemscope itemtype="http://schema.org/ImageGallery">
	 


&lt;/div>
&lt;/p>
&lt;h5 id="full-citation-dataset-with-doi">Full citation (dataset) with DOI&lt;/h5>
&lt;p>Wenclik, L., &amp;amp; Touya, G. (2024). EyeCatchingMaps, a Dataset to Assess Saliency Models on Maps [Data set]. Zenodo. &lt;a href="https://doi.org/10.5281/zenodo.10619513">https://doi.org/10.5281/zenodo.10619513&lt;/a>&lt;/p>
&lt;h5 id="related-articles">Related articles&lt;/h5>
&lt;p>Wenclik, L. and Touya, G. (2024) EyeCatchingMaps, a Dataset to Assess Saliency Models on Maps, AGILE GIScience Ser., 5, 51, &lt;a href="https://doi.org/10.5194/agile-giss-5-51-2024">https://doi.org/10.5194/agile-giss-5-51-2024&lt;/a>&lt;/p>
&lt;h5 id="related-links">Related links&lt;/h5>
&lt;p>&lt;a href="https://agile-giss.copernicus.org/articles/5/51/2024/">https://agile-giss.copernicus.org/articles/5/51/2024/&lt;/a>&lt;/p>


&lt;div class="box" >
 &lt;figure itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
 &lt;div class="img">
 &lt;img itemprop="thumbnail" src="https://MAP-VERSE.github.io/Repository/Repository/img/img05.jpg" alt="/Repository/img/img05.jpg"/>
 &lt;/div>
 &lt;a href="https://MAP-VERSE.github.io/Repository/Repository/img/img05.jpg" itemprop="contentUrl">&lt;/a>
 &lt;/figure>
&lt;/div></description></item><item><title>GeoEye</title><link>https://MAP-VERSE.github.io/Repository/post/dataset01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://MAP-VERSE.github.io/Repository/post/dataset01/</guid><description>&lt;h4 id="abstract">Abstract&lt;/h4>
&lt;p>&lt;a href="https://www.tandfonline.com/doi/abs/10.1080/15230406.2022.2153172">A geospatial image-based eye movement dataset called GeoEye, a publicly shared, widely available eye movement dataset. This dataset consists of 110 college- aged participants who freely viewed 500 images, including thematic maps, remote sensing images, and street view images, which could use for geospatial image saliency prediction and map user identification.&lt;/a>

&lt;link rel="stylesheet" href="https://MAP-VERSE.github.io/Repository/css/hugo-easy-gallery.css" />

&lt;div class="gallery caption-position-bottom caption-effect-fade hover-effect-zoom hover-transition" itemscope itemtype="http://schema.org/ImageGallery">
	 

&lt;/div>
&lt;/p>
&lt;h5 id="full-citation-dataset-with-doi">Full citation (dataset) with DOI&lt;/h5>
&lt;p>HE, ICE (2021). GeoEye dataset. figshare. Dataset.
&lt;a href="https://doi.org/10.6084/m9.figshare.14684214.v4">https://doi.org/10.6084/m9.figshare.14684214.v4&lt;/a>&lt;/p>
&lt;h5 id="related-articles">Related articles&lt;/h5>
&lt;p>He, B., Dong, W., Liao, H., Ying, Q., Shi, B., Liu, J., &amp;amp; Wang, Y. (2023). A geospatial image based eye movement dataset for cartography and GIS. Cartography and Geographic Information Science, 50(1), 96–111.
&lt;a href="https://doi.org/10.1080/15230406.2022.2153172">https://doi.org/10.1080/15230406.2022.2153172&lt;/a>&lt;/p>
&lt;h5 id="related-links">Related links&lt;/h5>
&lt;p>&lt;a href="https://www.tandfonline.com/doi/full/10.1080/15230406.2022.2153172">https://www.tandfonline.com/doi/full/10.1080/15230406.2022.2153172&lt;/a>&lt;/p>
&lt;!-- raw HTML omitted -->


&lt;div class="box" >
 &lt;figure itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
 &lt;div class="img">
 &lt;img itemprop="thumbnail" src="https://MAP-VERSE.github.io/Repository/Repository/img/img01.jpg" alt="/Repository/img/img01.jpg"/>
 &lt;/div>
 &lt;a href="https://MAP-VERSE.github.io/Repository/Repository/img/img01.jpg" itemprop="contentUrl">&lt;/a>
 &lt;/figure>
&lt;/div></description></item></channel></rss>