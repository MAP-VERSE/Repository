<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>MAP-VERSE</title>
<meta name=description content="Introduction Welcome to the MAP-VERSE: MAP usability - Validated Empirical Research by Systematic Evaluation! We are building a unique data discovery repository dedicated to advancing empirical research in map usability and spatial cognition. Our goal is to foster open science and collaboration by creating a centralized, queryable repository for metadata from open-access map usability studies through a living collection of publicly shared datasets recorded during geospatial tasks such as map reading, navigation."><meta name=author content="Cartographers & GISers"><script type=application/ld+json>{"@context":"http://schema.org","@type":"WebSite","name":"MAP-VERSE","url":"https:\/\/MAP-VERSE.github.io\/Repository\/"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Organization","name":"","url":"https:\/\/MAP-VERSE.github.io\/Repository\/"}</script><meta property="og:title" content="MAP-VERSE"><meta property="og:description" content="Introduction Welcome to the MAP-VERSE: MAP usability - Validated Empirical Research by Systematic Evaluation! We are building a unique data discovery repository dedicated to advancing empirical research in map usability and spatial cognition. Our goal is to foster open science and collaboration by creating a centralized, queryable repository for metadata from open-access map usability studies through a living collection of publicly shared datasets recorded during geospatial tasks such as map reading, navigation."><meta property="og:image" content="https://MAP-VERSE.github.io/Repository/img/mv.png"><meta property="og:url" content="https://MAP-VERSE.github.io/Repository/"><meta property="og:type" content="website"><meta property="og:site_name" content="MAP-VERSE"><meta name=twitter:title content="MAP-VERSE"><meta name=twitter:description content="Introduction Welcome to the MAP-VERSE: MAP usability - Validated Empirical Research by Systematic Evaluation! We are building a unique data discovery repository dedicated to advancing empirical …"><meta name=twitter:image content="https://MAP-VERSE.github.io/Repository/img/mv.png"><meta name=twitter:card content="summary_large_image"><link href=https://MAP-VERSE.github.io/Repository/img/mv.ico rel=icon type=image/x-icon><meta name=generator content="Hugo 0.130.0"><link rel=alternate href=https://MAP-VERSE.github.io/Repository/index.xml type=application/rss+xml title=MAP-VERSE><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.5.0/css/all.css integrity=sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css integrity=sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu crossorigin=anonymous><link rel=stylesheet href=https://MAP-VERSE.github.io/Repository/css/main.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic"><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"><link rel=stylesheet href=https://MAP-VERSE.github.io/Repository/css/highlight.min.css><link rel=stylesheet href=https://MAP-VERSE.github.io/Repository/css/codeblock.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css integrity=sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css integrity=sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R crossorigin=anonymous><meta name=google-site-verification content="HPJhWWOfmhFFZiW6tCPbN7cJb3AqY5jOxitjrWZ-oYM"><meta name=google-site-verification content="HPJhWWOfmhFFZiW6tCPbN7cJb3AqY5jOxitjrWZ-oYM"></head><body><nav class="navbar navbar-default navbar-fixed-top navbar-custom"><div class=container-fluid><div class=navbar-header><button type=button class=navbar-toggle data-toggle=collapse data-target=#main-navbar>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=https://MAP-VERSE.github.io/Repository/>MAP-VERSE</a></div><div class="collapse navbar-collapse" id=main-navbar><ul class="nav navbar-nav navbar-right"><li><a title=Homepage href=/Repository/>Homepage</a></li><li><a title=About href=/Repository/page/about/>About</a></li><li><a title=Keywords href=/Repository/tags>Keywords</a></li><li><a title=Tools href=/Repository/post/data_tools>Tools</a></li><li><a title=License href=/Repository/page/license_mit>License</a></li><li><a href=#modalSearch data-toggle=modal data-target=#modalSearch style=outline:none><span class="hidden-sm hidden-md hidden-lg">Search</span> <span id=searchGlyph class="glyphicon glyphicon-search"></span></a></li></ul></div><div class=avatar-container><div class=avatar-img-border><a title=MAP-VERSE href=https://MAP-VERSE.github.io/Repository/><img class=avatar-img src=https://MAP-VERSE.github.io/Repository/img/mv.png alt=MAP-VERSE></a></div></div></div></nav><div id=modalSearch class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><button type=button class=close data-dismiss=modal>&#215;</button><h4 class=modal-title>Search MAP-VERSE</h4></div><div class=modal-body><gcse:search></gcse:search></div><div class=modal-footer><button type=button class="btn btn-default" data-dismiss=modal>Close</button></div></div></div></div><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><div id=header-big-imgs data-num-img=1 data-img-src-1=https://MAP-VERSE.github.io/Repository/img/bg3.jpg></div><header class="header-section has-img"><div class="intro-header big-img" style=background-image:url(img/bg3.jpg)><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=page-heading><h1>MAP-VERSE</h1><hr class=small><span class=page-subheading>MAP usability - Validated Empirical Research by Systematic Evaluation</span></div></div></div></div><span class=img-desc style=display:none></span></div><div class="intro-header no-img"><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=page-heading><h1>MAP-VERSE</h1><hr class=small><span class=page-subheading>MAP usability - Validated Empirical Research by Systematic Evaluation</span></div></div></div></div></div></header><div role=main class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=well><h2 id=introduction>Introduction</h2><p>Welcome to the <strong>MAP-VERSE: MAP usability - Validated Empirical Research by Systematic Evaluation</strong>! We are building a unique data discovery repository dedicated to advancing empirical research in map usability and spatial cognition. Our goal is to foster open science and collaboration by creating a centralized, queryable repository for metadata from open-access map usability studies through a living collection of publicly shared datasets recorded during geospatial tasks such as map reading, navigation. By this way, we strive to</p><p><strong>encourage standardized reporting of user studies (minimum reporting standards)</strong></p><p><strong>showcase best practices in map reading studies</strong></p><p><strong>increase generalizability and reproducibility of findings</strong></p><p>This repository acts as a queryable metadata repository for open-access eye tracking, neuroimaging (EEG, fMRI), and human sensing (EDA, cardiovascular activity, skin temperature) datasets recorded during geospatial tasks, a.k.a. map reading user studies. The links to open datasets shared elsewhere (e.g., zenodo, harvard dataverse)</p></div><div class=posts-list><article class=post-preview><a href=https://MAP-VERSE.github.io/Repository/post/dataset02/><h2 class=post-title>CartoGAZE</h2><h3 class=post-subtitle>Visual Attention and Recognition Differences Based on Expertise in a Map Reading and Memorability Study</h3></a><p class=post-meta><span class=post-meta></span></p><div class=post-entry><h4 id=abstract>Abstract</h4><p><a href=https://www.mdpi.com/2220-9964/12/1/21>This study investigates how expert and novice map users’ attention is influenced by the map design characteristics of 2D web maps by building and sharing a framework to analyze large volumes of eye tracking data. In this context, we developed an automated area-of-interest (AOI) analysis framework to evaluate participants’ fixation durations, and to assess the influence of linear and polygonal map features on spatial memory. The dataset entitled CartoGAZE is publicly available.</a></p><link rel=stylesheet href=https://MAP-VERSE.github.io/Repository/css/hugo-easy-gallery.css><div class="gallery caption-position-bottom caption-effect-fade hover-effect-zoom hover-transition" itemscope itemtype=http://schema.org/ImageGallery></div><h5 id=full-citation-dataset-with-doi>Full citation (dataset) with DOI</h5><p>Keskin, Merve, 2023, &ldquo;CartoGAZE&rdquo;, <a href=https://doi.org/10.7910/DVN/ONIAZI>https://doi.org/10.7910/DVN/ONIAZI</a>, Harvard Dataverse, V1</p><h5 id=related-articles>Related articles</h5><p>Keskin M, Krassanakis V, Çöltekin A. Visual Attention and Recognition Differences Based on Expertise in a Map Reading and Memorability Study. ISPRS International Journal of Geo-Information. 2023; 12(1):21.</p><h5 id=related-links>Related links</h5><p><a href=https://www.mdpi.com/2220-9964/12/1/21>https://www.mdpi.com/2220-9964/12/1/21</a></p><p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/Repository/img/img02_01.jpg alt=/Repository/img/img02_01.jpg></div><a href=/Repository/img/img02_01.jpg itemprop=contentUrl></a></figure></div><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/Repository/img/img02_02.jpg alt=/Repository/img/img02_02.jpg></div><a href=/Repository/img/img02_02.jpg itemprop=contentUrl></a></figure></div></p></div><div class=blog-tags><a href=https://MAP-VERSE.github.io/Repository/tags/eye-tracking/>eye tracking</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/aoi/>AOI</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/spatial-memory/>spatial memory</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/memorability/>memorability</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/cartographic-usability/>cartographic usability</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/task-difficulty/>task difficulty</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/expertise/>expertise</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/eye-tracking-dataset/>eye tracking dataset</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/navigational-maps/>navigational maps</a>&nbsp;</div></article><article class=post-preview><a href=https://MAP-VERSE.github.io/Repository/post/dataset05/><h2 class=post-title>EyeCatchingMaps</h2><h3 class=post-subtitle>EyeCatchingMaps, a Dataset to Assess Saliency Models on Maps</h3></a><p class=post-meta><span class=post-meta></span></p><div class=post-entry><h4 id=abstract>Abstract</h4><p><a href=https://agile-giss.copernicus.org/articles/5/51/2024/>Saliency models try to predict the gaze behaviour of people in the first seconds of their observation of an image. To assess how much these models can perform to predict saliency in maps, we lack a ground truth to compare to. This paper proposes EyeCatchingMaps, an open dataset that can be used to benchmark saliency models for maps. The dataset has been obtained by recording the gaze of participants looking at different maps for 3 seconds with an eye-tracker. The use of EyeCatchingMaps is demonstrated by comparing two different saliency models from the literature to the real saliency maps derived from people’s gaze.</a>
<link rel=stylesheet href=https://MAP-VERSE.github.io/Repository/css/hugo-easy-gallery.css><div class="gallery caption-position-bottom caption-effect-fade hover-effect-zoom hover-transition" itemscope itemtype=http://schema.org/ImageGallery></div></p><h5 id=full-citation-dataset-with-doi>Full citation (dataset) with DOI</h5><p>Wenclik, L., & Touya, G. (2024). EyeCatchingMaps, a Dataset to Assess Saliency Models on Maps [Data set]. Zenodo. <a href=https://doi.org/10.5281/zenodo.10619513>https://doi.org/10.5281/zenodo.10619513</a></p><h5 id=related-articles>Related articles</h5><p>Wenclik, L. and Touya, G. (2024) EyeCatchingMaps, a Dataset to Assess Saliency Models on Maps, AGILE GIScience Ser., 5, 51, <a href=https://doi.org/10.5194/agile-giss-5-51-2024>https://doi.org/10.5194/agile-giss-5-51-2024</a></p><h5 id=related-links>Related links</h5><p><a href=https://agile-giss.copernicus.org/articles/5/51/2024/>https://agile-giss.copernicus.org/articles/5/51/2024/</a></p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/Repository/img/img05.jpg alt=/Repository/img/img05.jpg></div><a href=/Repository/img/img05.jpg itemprop=contentUrl></a></figure></div></div><div class=blog-tags><a href=https://MAP-VERSE.github.io/Repository/tags/cartography/>cartography</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/maps/>maps</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/eye-tracking/>eye-tracking</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/saliency/>saliency</a>&nbsp;</div></article><article class=post-preview><a href=https://MAP-VERSE.github.io/Repository/post/dataset06/><h2 class=post-title>EyeMouseMap</h2><h3 class=post-subtitle>Quantifying map user response differences between gaze and cursor activity during searching cartographic point symbols</h3></a><p class=post-meta><span class=post-meta></span></p><div class=post-entry><h4 id=abstract>Abstract</h4><p><a href="https://eurocarto2024.org/wp-content/uploads/2024/10/EC24_workshop_online-user-experiments_proceedings.pdf#page=9">The examination of both perceptual and cognitive issues related to map reading requires the performance of experimental procedures that aim to measure map user response under free viewing or task-oriented conditions. Map user reaction can be modeled by considering data collected utilizing several experimental techniques (e.g., reaction time and response accuracy measurements). However, it is of great importance to explore experimental frameworks that can be executed remotely. The present study aims to present a work in progress that aims to compare gaze and cursor activity during the execution of a typical map reading task. In more detail, we implement a lab-based experiment which concurrently captures both eye and (computer) mouse movements while searching for specific point symbols on cartographic backgrounds. The experiment is based on the use of the visual stimuli and point target symbols presented by Pappa & Krassanakis (2022). The aforementioned study was implemented utilizing remote (online) mouse tracking techniques. In the framework of the current work, the overarching goal is to explore quantitative measures that are able to describe individual and/or aggregated visual search behavioral differences. Considering the aggregated statistical grayscale heatmaps produced by both experimental studies, we plan to use the Jaccard index, the Dice coefficient and the BF score in order to perform comparisons between gaze and cursor activity, as well as between the mouse movement data produced under both conditions (lab-based and online). In addition, we will compute the GraphGazeD (Liaskos & Krassanakis, 2024) metric towards describing existing visual perception differences. The process of data analysis will be fully automatized using Python programming language and MATLAB software.</a>
<link rel=stylesheet href=https://MAP-VERSE.github.io/Repository/css/hugo-easy-gallery.css><div class="gallery caption-position-bottom caption-effect-fade hover-effect-zoom hover-transition" itemscope itemtype=http://schema.org/ImageGallery></div></p><h5 id=full-citation-dataset-with-doi>Full citation (dataset) with DOI</h5><p>Vlachou, A., Pappa, A., Liaskos, D., & Krassanakis, V. (2024). EyeMouseMap [Data set]. Zenodo. <a href=https://doi.org/10.5281/zenodo.13929730>https://doi.org/10.5281/zenodo.13929730</a></p><h5 id=related-articles>Related articles</h5><p>Vlachou A., Liaskos, D., & Krassanakis, V. (2024). Quantifying map user response differences between gaze and cursor activity during searching cartographic point symbols. Online User Experiments: Seeing What Map Users See without Seeing Them (Pre-conference Workshop, EuroCarto 2024). Available at: <a href="https://eurocarto2024.org/wp-content/uploads/2024/10/EC24_workshop_online-user-experiments_proceedings.pdf#page=9">https://eurocarto2024.org/wp-content/uploads/2024/10/EC24_workshop_online-user-experiments_proceedings.pdf#page=9</a> Pappa, A., &</p><p>Krassanakis, V. (2022). Examining the preattentive effect on cartographic backgrounds utilizing remote mouse tracking. Abstracts of the ICA, 5, 111. (EuroCarto 2022 Conference). <a href=https://doi.org/10.5194/ica-abs-5-111-2022>https://doi.org/10.5194/ica-abs-5-111-2022</a></p><h5 id=related-links>Related links</h5><p><a href=https://doi.org/10.5194/ica-abs-5-111-2022>https://doi.org/10.5194/ica-abs-5-111-2022</a></p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/Repository/img/img06.jpg alt=/Repository/img/img06.jpg></div><a href=/Repository/img/img06.jpg itemprop=contentUrl></a></figure></div></div><div class=blog-tags><a href=https://MAP-VERSE.github.io/Repository/tags/eye-tracking/>eye tracking</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/mouse-tracking/>mouse tracking</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/comparison-metrics/>comparison metrics</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/lab-based-and-online-user-studies/>lab-based and online user studies</a>&nbsp;</div></article><article class=post-preview><a href=https://MAP-VERSE.github.io/Repository/post/dataset09/><h2 class=post-title>Fixations & Saccades metadata</h2><h3 class=post-subtitle>Animating Cartographic Meaning - Unveiling the Impact of Pictorial Symbol Motion Speed in Preattentive Processing</h3></a><p class=post-meta><span class=post-meta></span></p><div class=post-entry><h4 id=abstract>Abstract</h4><p><a href=https://www.mdpi.com/2220-9964/13/4/118>The primary objective of this study is to assess how the motion of dynamic point symbols impacts preattentive processing on a map. Specifically, it involves identifying the motion velocity parameters for cartographic animated pictorial symbols that contribute to the preattentive perception of the target symbols. We created five pictorial symbols, each accompanied by a unique animation tailored to convey the meaning associated with each symbol. The animation dynamics of symbols on the administrative map were distributed across arithmetic, logarithmic, and exponential scales. Eye-tracking technology was utilized to explain the user’s visual attention. The key findings reveal that, although movement does not uniformly hold the same designation in cartographic communication, it could guide user attention to identify the value peaks in quantitative map visualization. Motion velocity enhances the salience of animated symbols, making them stand out, not only against static elements but also against other animated distractors. Additionally, motion distributions between symbol classes based on exponential or arithmetic scales were identified as the most successful. Nevertheless, certain types of motion, such as rotational, do not perform well with pictorial symbols, even on the most effective motion distribution scale.</a></p><link rel=stylesheet href=https://MAP-VERSE.github.io/Repository/css/hugo-easy-gallery.css><div class="gallery caption-position-bottom caption-effect-fade hover-effect-zoom hover-transition" itemscope itemtype=http://schema.org/ImageGallery></div><h5 id=full-citation-dataset-with-doi>Full citation (dataset) with DOI</h5><p>Cybulski, Paweł, 2023, &ldquo;Fixations & Saccades metadata (NCN SONATA 16 &ldquo;Preattentive attributes of dynamic point symbols in quantitative mapping&rdquo;)&rdquo;, <a href=https://doi.org/10.7910/DVN/YHYXLI>https://doi.org/10.7910/DVN/YHYXLI</a>, Harvard Dataverse, V1</p><h5 id=related-articles>Related articles</h5><p>Cybulski P. Animating Cartographic Meaning: Unveiling the Impact of Pictorial Symbol Motion Speed in Preattentive Processing. ISPRS International Journal of Geo-Information. 2024; 13(4):118. <a href=https://doi.org/10.3390/ijgi13040118>https://doi.org/10.3390/ijgi13040118</a></p><h5 id=related-links>Related links</h5><p><a href=https://www.mdpi.com/2220-9964/13/4/118>https://www.mdpi.com/2220-9964/13/4/118</a></p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/Repository/img/img09.jpg alt=/Repository/img/img09.jpg></div><a href=/Repository/img/img09.jpg itemprop=contentUrl></a></figure></div></div><div class=blog-tags><a href=https://MAP-VERSE.github.io/Repository/tags/animated-symbols/>animated symbols</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/preattentive-processing/>preattentive processing</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/pictorial-symbols/>pictorial symbols</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/motion-distribution/>motion distribution</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/cartographic-design/>cartographic design</a>&nbsp;</div></article><article class=post-preview><a href=https://MAP-VERSE.github.io/Repository/post/dataset01/><h2 class=post-title>GeoEye</h2><h3 class=post-subtitle>BNU500 A geospatial image based eye movement dataset for cartography and GIS</h3></a><p class=post-meta><span class=post-meta></span></p><div class=post-entry><h4 id=abstract>Abstract</h4><p><a href=https://www.tandfonline.com/doi/abs/10.1080/15230406.2022.2153172>A geospatial image-based eye movement dataset called GeoEye, a publicly shared, widely available eye movement dataset. This dataset consists of 110 college- aged participants who freely viewed 500 images, including thematic maps, remote sensing images, and street view images, which could use for geospatial image saliency prediction and map user identification.</a>
<link rel=stylesheet href=https://MAP-VERSE.github.io/Repository/css/hugo-easy-gallery.css><div class="gallery caption-position-bottom caption-effect-fade hover-effect-zoom hover-transition" itemscope itemtype=http://schema.org/ImageGallery></div></p><h5 id=full-citation-dataset-with-doi>Full citation (dataset) with DOI</h5><p>HE, ICE (2021). GeoEye dataset. figshare. Dataset.
<a href=https://doi.org/10.6084/m9.figshare.14684214.v4>https://doi.org/10.6084/m9.figshare.14684214.v4</a></p><h5 id=related-articles>Related articles</h5><p>He, B., Dong, W., Liao, H., Ying, Q., Shi, B., Liu, J., & Wang, Y. (2023). A geospatial image based eye movement dataset for cartography and GIS. Cartography and Geographic Information Science, 50(1), 96–111.
<a href=https://doi.org/10.1080/15230406.2022.2153172>https://doi.org/10.1080/15230406.2022.2153172</a></p><h5 id=related-links>Related links</h5><p><a href=https://www.tandfonline.com/doi/full/10.1080/15230406.2022.2153172>https://www.tandfonline.com/doi/full/10.1080/15230406.2022.2153172</a></p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/Repository/img/img01.jpg alt=/Repository/img/img01.jpg></div><a href=/Repository/img/img01.jpg itemprop=contentUrl></a></figure></div></div><div class=blog-tags><a href=https://MAP-VERSE.github.io/Repository/tags/eye-movement-dataset/>eye movement dataset</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/geospatial-image/>geospatial image</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/cartography/>cartography</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/gis/>GIS</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/visual-saliency-detection/>visual saliency detection</a>&nbsp;</div></article><article class=post-preview><a href=https://MAP-VERSE.github.io/Repository/post/dataset07/><h2 class=post-title>Map activities recognition</h2><h3 class=post-subtitle>Recognition of map activities using eye tracking and EEG data</h3></a><p class=post-meta><span class=post-meta></span></p><div class=post-entry><h4 id=abstract>Abstract</h4><p><a href=https://www.tandfonline.com/doi/abs/10.1080/13658816.2024.2309188>Recognizing the activities being performed on a map is crucial for adaptive map design based on user context. Despite eye tracking (ET) demonstrating potential in recognizing map activities and electroencephalography (EEG) measuring map users’ cognitive load, no studies have yet combined ET and EEG for recognition of the user’s activity on maps. Our study collected participants’ ET and EEG data during four types of map activities. After feature extraction and selection, we trained LightGBM (light Gradient-Boosting Machine) to classify these activities, and achieved 88.0% accuracy when combining ET and EEG features in the entire map usage trial, which is higher than using ET (85.9%) or EEG (53.9%) alone. Acceptable recognition accuracy could also be achieved with the early time windows (73.1% when using the first 3 seconds). Saccade features of ET were the most important for differentiating map activities, indicating selective map content for different tasks. Our findings demonstrate the feasibility and advantages of combining ET and EEG for activity recognition in map use. The results not only improve our understanding of visual patterns and cognitive processes in map use, but also enable the design of adaptive maps that can automatically adapt to the activities a map user is performing.</a>
<link rel=stylesheet href=https://MAP-VERSE.github.io/Repository/css/hugo-easy-gallery.css><div class="gallery caption-position-bottom caption-effect-fade hover-effect-zoom hover-transition" itemscope itemtype=http://schema.org/ImageGallery></div></p><h5 id=full-citation-dataset-with-doi>Full citation (dataset) with DOI</h5><p>Qin, Tong; Fias, Wim; Weghe, Nico Van de; Huang, Haosheng (2023). Map activity recognition dataset. figshare. Dataset. <a href=https://doi.org/10.6084/m9.figshare.23805027.v2>https://doi.org/10.6084/m9.figshare.23805027.v2</a></p><h5 id=related-articles>Related articles</h5><p>Qin, T., Fias, W., Van de Weghe, N., & Huang, H. (2024). Recognition of map activities using eye tracking and EEG data. International Journal of Geographical Information Science, 38(3), 550–576. <a href=https://doi.org/10.1080/13658816.2024.2309188>https://doi.org/10.1080/13658816.2024.2309188</a></p><h5 id=related-links>Related links</h5><p><a href=https://www.tandfonline.com/doi/full/10.1080/13658816.2024.2309188>https://www.tandfonline.com/doi/full/10.1080/13658816.2024.2309188</a></p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/Repository/img/img07.jpg alt=/Repository/img/img07.jpg></div><a href=/Repository/img/img07.jpg itemprop=contentUrl></a></figure></div></div><div class=blog-tags><a href=https://MAP-VERSE.github.io/Repository/tags/eye-tracking/>eye tracking</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/map-activities/>map activities</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/activity-recognition/>activity recognition</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/electroencephalography/>electroencephalography</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/map-adaptation/>map adaptation</a>&nbsp;</div></article><article class=post-preview><a href=https://MAP-VERSE.github.io/Repository/post/dataset04/><h2 class=post-title>MDHScomplexity</h2><h3 class=post-subtitle>Evaluating the perceived visual complexity of multidirectional hill-shading</h3></a><p class=post-meta><span class=post-meta></span></p><div class=post-entry><h4 id=abstract>Abstract</h4><p><a href=https://doi.org/10.24425/gac.2020.131085>Eye tracking recordings could reveal the visual behavior for different carto- graphic visualization techniques, such as hill-shading, while at the same time eye track- ing metrics (ETMs) can summarize the associated complexity levels in a concise and quantitative manner. In the present study, three different hill-shading methods, including: (i) the standard method based on ideal diffuse reflection, (ii) the Multidirectional Oblique- Weighted method – MDOW and (iii) the combination of a MDOW’s variation with stan- dard hill-shading, are evaluated and ranked in terms of their perceived visual complexity. The performed examination is based on both eye tracking techniques and expert judgement procedures. A weighted combination of basic ETMs has been used, implemented by the Landscape Rating Index – LRI. The weights resulted from an experts’ judgement process where the opinions of experts in geoinformatics, cartography, geovisualization, experimen- tal psychology, cognitive science, neuroscience and eye tracking were analyzed. Fifteen (15) individuals participated in an eye tracking experiment with hill-shading images produced by the three methods under evaluation, while 41 experts participated in an online questionnaire in order to collect all the analysis data. The final evaluation was based on the computation of three LRI models. The outcomes indicate that implementing hill-shading with more than one light sources results in similar perceptual behaviors, allowing for a seamless exploita- tion of the advantages of using multidirectional illumination.</a>
<link rel=stylesheet href=https://MAP-VERSE.github.io/Repository/css/hugo-easy-gallery.css><div class="gallery caption-position-bottom caption-effect-fade hover-effect-zoom hover-transition" itemscope itemtype=http://schema.org/ImageGallery></div></p><h5 id=full-citation-dataset-with-doi>Full citation (dataset) with DOI</h5><p>Tzelepis, N., Krassanakis, V., Kaliakouda, A., Misthos, L.-M., & Nakos, B. (2020). MDHScomplexity (Version v1) [Data set]. Zenodo. <a href=https://doi.org/10.5281/zenodo.14680770>https://doi.org/10.5281/zenodo.14680770</a></p><h5 id=related-articles>Related articles</h5><p>Tzelepis, N., Kaliakouda, A., Krassanakis, V., Misthos, L. M., & Nakos, B. (2020). Evaluating the perceived visual complexity of multidirectional hill-shading. Geodesy and Cartography, 69(2), 161-172. <a href=https://doi.org/10.24425/gac.2020.131085>https://doi.org/10.24425/gac.2020.131085</a></p><h5 id=related-links>Related links</h5><p><a href=http://users.ntua.gr/niktzel/gc_paper_data.zip>http://users.ntua.gr/niktzel/gc_paper_data.zip</a><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/Repository/img/img04.jpg alt=/Repository/img/img04.jpg></div><a href=/Repository/img/img04.jpg itemprop=contentUrl></a></figure></div></p></div><div class=blog-tags><a href=https://MAP-VERSE.github.io/Repository/tags/multidirectional-hill-shading/>multidirectional hill-shading</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/visual-complexity/>visual complexity</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/eye-movement-analysis/>eye movement analysis</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/expert-judgement-process/>expert judgement process</a>&nbsp;</div></article><article class=post-preview><a href=https://MAP-VERSE.github.io/Repository/post/dataset03/><h2 class=post-title>OnMapGaze</h2><h3 class=post-subtitle>OnMapGaze and GraphGazeD - A Gaze Dataset and a Graph-Based Metric for Modeling Visual Perception Differences in Cartographic Backgrounds Used in Online Map Services</h3></a><p class=post-meta><span class=post-meta></span></p><div class=post-entry><h4 id=abstract>Abstract</h4><p><a href=https://www.mdpi.com/2414-4088/8/6/49>In the present study, a new eye-tracking dataset (OnMapGaze) and a graph-based metric (GraphGazeD) for modeling visual perception differences are introduced. The dataset includes both experimental and analyzed gaze data collected during the observation of different cartographic backgrounds used in five online map services, including Google Maps, Wikimedia, Bing Maps, ESRI, and OSM, at three different zoom levels (12z, 14z, and 16z). The computation of the new metric is based on the utilization of aggregated gaze behavior data. Our dataset aims to serve as an objective ground truth for feeding artificial intelligence (AI) algorithms and developing computational models for predicting visual behavior during map reading. Both the OnMapGaze dataset and the source code for computing the GraphGazeD metric are freely distributed to the scientific community.</a>
<link rel=stylesheet href=https://MAP-VERSE.github.io/Repository/css/hugo-easy-gallery.css><div class="gallery caption-position-bottom caption-effect-fade hover-effect-zoom hover-transition" itemscope itemtype=http://schema.org/ImageGallery></div></p><h5 id=full-citation-dataset-with-doi>Full citation (dataset) with DOI</h5><p>Liaskos D, Krassanakis V. OnMapGaze and GraphGazeD: A Gaze Dataset and a Graph-Based Metric for Modeling Visual Perception Differences in Cartographic Backgrounds Used in Online Map Services. Multimodal Technologies and Interaction. 2024; 8(6):49. <a href=https://doi.org/10.3390/mti8060049>https://doi.org/10.3390/mti8060049</a></p><h5 id=related-articles>Related articles</h5><p>Keskin M, Krassanakis V, Çöltekin A. Visual Attention and Recognition Differences Based on Expertise in a Map Reading and Memorability Study. ISPRS International Journal of Geo-Information. 2023; 12(1):21.</p><h5 id=related-links>Related links</h5><p><a href=https://www.mdpi.com/2414-4088/8/6/49>https://www.mdpi.com/2414-4088/8/6/49</a></p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/Repository/img/img03.jpg alt=/Repository/img/img03.jpg></div><a href=/Repository/img/img03.jpg itemprop=contentUrl></a></figure></div></div><div class=blog-tags><a href=https://MAP-VERSE.github.io/Repository/tags/eye-tracking/>eye tracking</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/gaze-dataset/>gaze dataset</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/graph-based-metric/>graph-based metric</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/visual-perception-differences/>visual perception differences</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/statistical-grayscale-heatmaps/>statistical grayscale heatmaps</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/cartographic-backgrounds/>cartographic backgrounds</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/online-map-services/>online map services</a>&nbsp;</div></article><article class=post-preview><a href=https://MAP-VERSE.github.io/Repository/post/dataset08/><h2 class=post-title>Rymarkiewicz et al. (2024)</h2><h3 class=post-subtitle>Measuring Efficiency and Accuracy in Locating Symbols on Mobile Maps Using Eye Tracking</h3></a><p class=post-meta><span class=post-meta></span></p><div class=post-entry><h4 id=abstract>Abstract</h4><p><a href=https://www.mdpi.com/2220-9964/13/2/42>This study investigated the impact of smartphone usage frequency on the effectiveness and accuracy of symbol location in a variety of spatial contexts on mobile maps using eye-tracking technology while utilizing the example of Mapy.cz. The scanning speed and symbol detection were also considered. The use of mobile applications for navigation is discussed, emphasizing their popularity and convenience of use. The importance of eye tracking as a valuable tool for testing the usability of cartographic products, enabling the assessment of users’ visual strategies and their ability to memorize information, was highlighted. The frequency of smartphone use has been shown to be an important factor in users’ ability to locate symbols in different spatial contexts. Everyday smartphone users have shown higher accuracy and efficiency in image processing, suggesting a potential link between habitual smartphone use and increased efficiency in mapping tasks. Participants who were dissatisfied with the legibility of a map looked longer at the symbols, suggesting that they put extra cognitive effort into decoding the symbols. In the present study, gender differences in pupil size were also observed during the study. Women consistently showed a larger pupil diameter, potentially indicating greater cognitive load on the participants.</a></p><link rel=stylesheet href=https://MAP-VERSE.github.io/Repository/css/hugo-easy-gallery.css><div class="gallery caption-position-bottom caption-effect-fade hover-effect-zoom hover-transition" itemscope itemtype=http://schema.org/ImageGallery></div><h5 id=full-citation-dataset-with-doi>Full citation (dataset) with DOI</h5><p>Horbiński, Tymoteusz, 2024, &ldquo;Efficiency and accuracy in locating symbols within diverse spatial contexts on mobile maps using eye-tracking technology on the example of the Mapy.cz&rdquo;, <a href=https://doi.org/10.7910/DVN/DZUFJ1>https://doi.org/10.7910/DVN/DZUFJ1</a>, Harvard Dataverse, V1</p><h5 id=related-articles>Related articles</h5><p>Rymarkiewicz W, Cybulski P, Horbiński T. Measuring Efficiency and Accuracy in Locating Symbols on Mobile Maps Using Eye Tracking. ISPRS International Journal of Geo-Information. 2024; 13(2):42. <a href=https://doi.org/10.3390/ijgi13020042>https://doi.org/10.3390/ijgi13020042</a></p><h5 id=related-links>Related links</h5><p><a href=https://www.mdpi.com/2220-9964/13/2/42>https://www.mdpi.com/2220-9964/13/2/42</a><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/Repository/img/img08.jpg alt=/Repository/img/img08.jpg></div><a href=/Repository/img/img08.jpg itemprop=contentUrl></a></figure></div></p></div><div class=blog-tags><a href=https://MAP-VERSE.github.io/Repository/tags/eye-tracking/>eye tracking</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/mobile-maps/>mobile maps</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/daily-smartphone-usage/>daily smartphone usage</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/spatial-contexts/>spatial contexts</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/locating-symbols/>locating symbols</a>&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/tags/mobile-navigation/>mobile navigation</a>&nbsp;</div></article><article class=post-preview><a href=https://MAP-VERSE.github.io/Repository/post/data_tools/><h2 class=post-title>Useful tools for datasets</h2></a><p class=post-meta><span class=post-meta></span></p><div class=post-entry><h4 id=links>Links</h4><p><a href=https://microsite.geo.uzh.ch/icacogvis/resources.html>https://microsite.geo.uzh.ch/icacogvis/resources.html</a></p><p><a href=https://eyetracking.upol.cz/tools/>https://eyetracking.upol.cz/tools/</a></p><p><a href=https://geogaze.ethz.ch/>https://geogaze.ethz.ch/</a></p><p><a href=https://sites.google.com/site/vassilioskrassanakis>https://sites.google.com/site/vassilioskrassanakis</a></p><h4 id=papers>Papers</h4><p><a href=https://www.mdpi.com/2673-6470/3/2/9>https://www.mdpi.com/2673-6470/3/2/9</a></p></div></article></div></div></div></div><div class=page-meta></div><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center footer-links"><li><a href=https://github.com/MAP-VERSE/Repository title=GitHub><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href=/Repository/index.xml title=RSS><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="credits copyright text-muted">Cartographers & GISers
&nbsp;&bull;&nbsp;&copy;
2024 - 2025
&nbsp;&bull;&nbsp;
<a href=https://MAP-VERSE.github.io/Repository/>MAP-VERSE</a></p><p class="credits theme-by text-muted"><a href=https://gohugo.io>Hugo v0.130.0</a> powered &nbsp;&bull;&nbsp; Theme <a href=https://github.com/halogenica/beautifulhugo>Beautiful Hugo</a> adapted from <a href=https://deanattali.com/beautiful-jekyll/>Beautiful Jekyll</a></p></div></div></div></footer><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script src=https://code.jquery.com/jquery-3.7.0.slim.min.js integrity=sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js integrity=sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd crossorigin=anonymous></script><script src=https://MAP-VERSE.github.io/Repository/js/main.js></script><script src=https://MAP-VERSE.github.io/Repository/js/highlight.min.js></script><script>hljs.initHighlightingOnLoad()</script><script>$(document).ready(function(){$("pre.chroma").css("padding","0")})</script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js integrity=sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js integrity=sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q crossorigin=anonymous></script><script src=https://MAP-VERSE.github.io/Repository/js/load-photoswipe.js></script><script>(function(){var t,n="6594bf4f7a48445fa",e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="https://cse.google.com/cse.js?cx="+n,t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>