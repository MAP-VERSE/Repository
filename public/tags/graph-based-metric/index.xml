<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Graph-Based Metric on Open Metadata Repository for Map Usability</title>
    <link>https://Freshorange2.github.io/Open-Metadata-Platform-for-Map-Usability_v2/tags/graph-based-metric/</link>
    <description>Recent content in Graph-Based Metric on Open Metadata Repository for Map Usability</description>
    <generator>Hugo</generator>
    <language>en</language>
    <atom:link href="https://Freshorange2.github.io/Open-Metadata-Platform-for-Map-Usability_v2/tags/graph-based-metric/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>OnMapGaze</title>
      <link>https://Freshorange2.github.io/Open-Metadata-Platform-for-Map-Usability_v2/post/dataset03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://Freshorange2.github.io/Open-Metadata-Platform-for-Map-Usability_v2/post/dataset03/</guid>
      <description>&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.mdpi.com/2414-4088/8/6/49&#34;&gt;In the present study, a new eye-tracking dataset (OnMapGaze) and a graph-based metric (GraphGazeD) for modeling visual perception differences are introduced. The dataset includes both experimental and analyzed gaze data collected during the observation of different cartographic backgrounds used in five online map services, including Google Maps, Wikimedia, Bing Maps, ESRI, and OSM, at three different zoom levels (12z, 14z, and 16z). The computation of the new metric is based on the utilization of aggregated gaze behavior data. Our dataset aims to serve as an objective ground truth for feeding artificial intelligence (AI) algorithms and developing computational models for predicting visual behavior during map reading. Both the OnMapGaze dataset and the source code for computing the GraphGazeD metric are freely distributed to the scientific community.&lt;/a&gt;&#xA;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://Freshorange2.github.io/Open-Metadata-Platform-for-Map-Usability_v2/css/hugo-easy-gallery.css&#34; /&gt;&#xA;&#xA;&lt;div class=&#34;gallery caption-position-bottom caption-effect-fade hover-effect-zoom hover-transition&#34; itemscope itemtype=&#34;http://schema.org/ImageGallery&#34;&gt;&#xA;&#x9;  &#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&lt;/p&gt;&#xA;&lt;h5 id=&#34;full-citation-dataset-with-doi&#34;&gt;Full citation (dataset) with DOI&lt;/h5&gt;&#xA;&lt;p&gt;Liaskos D, Krassanakis V. OnMapGaze and GraphGazeD: A Gaze Dataset and a Graph-Based Metric for Modeling Visual Perception Differences in Cartographic Backgrounds Used in Online Map Services. Multimodal Technologies and Interaction. 2024; 8(6):49. &lt;a href=&#34;https://doi.org/10.3390/mti8060049&#34;&gt;https://doi.org/10.3390/mti8060049&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h5 id=&#34;related-articles&#34;&gt;Related articles&lt;/h5&gt;&#xA;&lt;p&gt;Keskin M, Krassanakis V, Çöltekin A. Visual Attention and Recognition Differences Based on Expertise in a Map Reading and Memorability Study. ISPRS International Journal of Geo-Information. 2023; 12(1):21.&lt;/p&gt;&#xA;&lt;h5 id=&#34;related-links&#34;&gt;Related links&lt;/h5&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.mdpi.com/2414-4088/8/6/49&#34;&gt;https://www.mdpi.com/2414-4088/8/6/49&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;box&#34; &gt;&#xA;  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;&#xA;    &lt;div class=&#34;img&#34;&gt;&#xA;      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://Freshorange2.github.io/Open-Metadata-Platform-for-Map-Usability_v2/Open-Metadata-Platform-for-Map-Usability_v2/img/img03.jpg&#34; alt=&#34;/Open-Metadata-Platform-for-Map-Usability_v2/img/img03.jpg&#34;/&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;a href=&#34;https://Freshorange2.github.io/Open-Metadata-Platform-for-Map-Usability_v2/Open-Metadata-Platform-for-Map-Usability_v2/img/img03.jpg&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;&#xA;  &lt;/figure&gt;&#xA;&lt;/div&gt;</description>
    </item>
  </channel>
</rss>
